{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "from pymongo import MongoClient\n",
    "import certifi, re\n",
    "import warnings\n",
    "from datetime import datetime, timezone  # Ensure timezone is imported\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con = \"mongodb+srv://anjalijha1507:U54OU4PFxPYlVc4S@youtubedata.shzzp.mongodb.net/?retryWrites=true&w=majority&appName=YoutubeData\"\n",
    "# Create a MongoClient instance with CA bundle specified\n",
    "client = MongoClient(con, tls=True, tlsCAFile=certifi.where())\n",
    "\n",
    "# Attempt to get server information to confirm connection\n",
    "client.server_info()  # Forces a call to the server\n",
    "print(\"Successfully connected to MongoDB.\")\n",
    "\n",
    "# # Access a specific database (replace 'test' with your database name)\n",
    "db = client['Project1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = build('youtube', 'v3', developerKey='AIzaSyBPJ64uexibg77DCSd6rSGU8loyOTvndjI')\n",
    "# user_input = input('Enter the start date in YYYY-MM-DD format (data will be fetched from this date onwards): ')\n",
    "user_input = '2022-12-01'\n",
    "try:\n",
    "    start_date = datetime.strptime(user_input, '%Y-%m-%d').replace(tzinfo=timezone.utc) if user_input else None\n",
    "except ValueError:\n",
    "    print('Invalid date format. Please use YYYY-MM-DD format.')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_id_from_video_url(video_url):\n",
    "    try:\n",
    "        # Extract the video ID from the URL using regular expression\n",
    "        video_id_match = re.search(r'(?:youtu.be\\/|youtube.com\\/(?:v|e(?:mbed)?)\\/|youtube.com\\/watch\\?v=)([a-zA-Z0-9_-]{11})', video_url)\n",
    "        \n",
    "        if video_id_match:\n",
    "            video_id = video_id_match.group(1)\n",
    "            print(f\"Video ID extracted: {video_id}\")\n",
    "\n",
    "            # Request to get video details\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet\",\n",
    "                id=video_id\n",
    "            )\n",
    "\n",
    "            # Execute the request\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract channel ID from the response\n",
    "            if 'items' in response and len(response['items']) > 0:\n",
    "                channel_id = response['items'][0]['snippet']['channelId']\n",
    "                print(f\"Channel ID for the video is: {channel_id}\")\n",
    "                return channel_id\n",
    "            else:\n",
    "                print(\"No channel found for this video.\")\n",
    "        else:\n",
    "            print(\"Invalid YouTube video URL.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def get_video_statistics(video_id):\n",
    "    try:\n",
    "        video_response = youtube.videos().list(part='statistics,contentDetails', id=video_id).execute()\n",
    "        video_info = video_response.get('items', [])[0] if 'items' in video_response else {}\n",
    "        stats = video_info.get('statistics', {})\n",
    "        details = video_info.get('contentDetails', {})\n",
    "        \n",
    "        likes = int(stats.get('likeCount', 0))\n",
    "        comments = int(stats.get('commentCount', 0))\n",
    "        duration = details.get('duration', 'PT0S')\n",
    "        duration_seconds = parse_duration_to_seconds(duration)\n",
    "        \n",
    "        return likes, comments, duration_seconds\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching video data for ID {video_id}: {e}')\n",
    "        return 0, 0, 0\n",
    "\n",
    "def parse_duration_to_seconds(duration):\n",
    "    try:\n",
    "        duration = isodate.parse_duration(duration)\n",
    "        return int(duration.total_seconds())\n",
    "    except Exception as e:\n",
    "        print(f'Error parsing duration {duration}: {e}')\n",
    "        return 0\n",
    "\n",
    "def get_channel_statistics(channel_id, user_start_date):\n",
    "    try:\n",
    "\n",
    "        # Check if the channel_id already exists in MongoDB\n",
    "        existing_channel = db['youtube_channel_data'].find_one({'channel_id': channel_id})\n",
    "        if existing_channel:\n",
    "            print(f\"Channel ID {channel_id} already exists. Skipping data fetch.\")\n",
    "            return  # Skip fetching data if it already exists\n",
    "\n",
    "        channel_response = youtube.channels().list(part='snippet,contentDetails,statistics', id=channel_id).execute()\n",
    "        channel_info = channel_response.get('items', [])[0] if 'items' in channel_response else {}\n",
    "        print(\"uuuuuuuu\")\n",
    "        # print(channel_info)\n",
    "        \n",
    "        if channel_info:\n",
    "            snippet = channel_info.get('snippet', {})\n",
    "            statistics = channel_info.get('statistics', {})\n",
    "            content_details = channel_info.get('contentDetails', {})\n",
    "            \n",
    "            # Fetch channel start date\n",
    "            channel_start_date = snippet.get('publishedAt', 'NA')\n",
    "            channel_start_date = datetime.fromisoformat(channel_start_date.replace('Z', '+00:00'))\n",
    "            \n",
    "            # Use the user start date for filtering\n",
    "            # start_date = user_start_date if user_start_date else channel_start_date\n",
    "            # start_date = channel_start_date\n",
    "            # Get upload playlist ID\n",
    "            uploads_playlist_id = content_details.get('relatedPlaylists', {}).get('uploads', 'NA')\n",
    "            \n",
    "            if uploads_playlist_id == 'NA':\n",
    "                print(f'No uploads playlist found for channel ID {channel_id}.')\n",
    "                return\n",
    "            \n",
    "            total_likes = 0\n",
    "            total_comments = 0\n",
    "            short_videos_count = 0\n",
    "            long_videos_count = 0\n",
    "            \n",
    "            next_page_token = None\n",
    "            while True:\n",
    "                playlist_items_response = youtube.playlistItems().list(\n",
    "                    part='contentDetails,snippet',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=50,\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                items = playlist_items_response.get('items', [])\n",
    "                for item in items:\n",
    "                    video_id = item['contentDetails']['videoId']\n",
    "                    video_upload_date = item['snippet'].get('publishedAt', 'NA')\n",
    "                    video_upload_date = datetime.fromisoformat(video_upload_date.replace('Z', '+00:00'))\n",
    "                    \n",
    "                    if video_upload_date >= start_date:\n",
    "                        likes, comments, duration_seconds = get_video_statistics(video_id)\n",
    "                        total_likes += likes\n",
    "                        total_comments += comments\n",
    "                        if duration_seconds < 60:\n",
    "                            short_videos_count += 1\n",
    "                        else:\n",
    "                            long_videos_count += 1\n",
    "                \n",
    "                next_page_token = playlist_items_response.get('nextPageToken')\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "            \n",
    "            # Define the nested structure\n",
    "            channel_data = {\n",
    "                'channel_id': channel_id,\n",
    "                'channel_details': {\n",
    "                    'channel_name': snippet.get('title', 'NA'),\n",
    "                    'channel_start_date': channel_start_date.isoformat(),\n",
    "                    'inception_date' : start_date.isoformat(),\n",
    "                    'total_no_of_videos': statistics.get('videoCount', 'NA'),\n",
    "                    'total_no_short_videos': short_videos_count,\n",
    "                    'total_no_long_videos': long_videos_count,\n",
    "                    'total_views': statistics.get('viewCount', 'NA'),\n",
    "                    'total_likes': total_likes,\n",
    "                    'total_comments': total_comments,\n",
    "                    'total_subscribers': statistics.get('subscriberCount', 'NA'),\n",
    "\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Insert or update the data in MongoDB\n",
    "            collection = db['youtube_channel_data']  # Replace with your collection name\n",
    "            collection.update_one(\n",
    "                {'channel_id': channel_id},  # Use channel_id as unique identifier\n",
    "                {'$set': channel_data},\n",
    "                upsert=True\n",
    "            )\n",
    "            print(channel_data)\n",
    "            print(f'Data for channel ID {channel_id} inserted/updated in MongoDB.')\n",
    "        else:\n",
    "            print(f'Channel with ID {channel_id} not found or no data available.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching channel data for ID {channel_id}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "link = 'https://www.youtube.com/watch?v=p7V4Aa7qEpw'\n",
    "CHANNEL_IDS = get_channel_id_from_video_url(link)\n",
    "get_channel_statistics(CHANNEL_IDS, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANNEL_IDS = ['UCpAbD88ier5LH7p4f1aFmyQ']\n",
    "# # Fetch statistics for each channel and store in MongoDB\n",
    "# for channel_id in CHANNEL_IDS:\n",
    "#     print(f'Channel with ID {channel_id}')\n",
    "#     get_channel_statistics(CHANNEL_IDS, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['youtube_channel_data'] \n",
    "# Define the fields to retrieve\n",
    "fields = {\n",
    "    'channel_id': 1,\n",
    "    'channel_details.channel_name': 1,\n",
    "    'channel_details.channel_start_date': 1,\n",
    "    'channel_details.inception_date': 1,\n",
    "    'channel_details.total_no_of_videos': 1,\n",
    "    'channel_details.total_no_short_videos': 1,\n",
    "    'channel_details.total_no_long_videos': 1,\n",
    "    'channel_details.total_views': 1,\n",
    "    'channel_details.total_likes': 1,\n",
    "    'channel_details.total_comments': 1,\n",
    "    'channel_details.total_subscribers': 1\n",
    "}\n",
    "\n",
    "# Fetch documents and project the required fields\n",
    "documents = collection.find({}, {field: 1 for field in fields})\n",
    "\n",
    "# Convert documents to a list of dictionaries\n",
    "data = list(documents)\n",
    "\n",
    "# Normalize nested data for DataFrame\n",
    "df_data = pd.json_normalize(data, sep='_')\n",
    "\n",
    "# Rename columns to remove 'channel_details_' prefix\n",
    "df_data.columns = df_data.columns.str.replace('channel_details_', '', regex=False)\n",
    "\n",
    "# Convert date fields to YYYY-MM-DD format\n",
    "date_columns = ['channel_start_date', 'inception_date']\n",
    "for column in date_columns:\n",
    "    # Convert to datetime, handling potential microseconds\n",
    "    df_data[column] = pd.to_datetime(df_data[column].str.replace(r'\\.\\d+', '', regex=True)).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Drop the '_id' column if it exists\n",
    "df_data.drop('_id', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Save to CSV\n",
    "df_data.to_csv(\"Raw_Youtube_API_DATA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"Raw_Youtube_API_DATA.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['total_views'] = pd.to_numeric(df_data['total_views'], errors=\"coerce\")\n",
    "df_data['total_likes'] = pd.to_numeric(df_data['total_likes'], errors=\"coerce\")\n",
    "df_data['total_comments'] = pd.to_numeric(df_data['total_comments'], errors=\"coerce\")\n",
    "df_data['total_subscribers'] = pd.to_numeric(df_data['total_subscribers'], errors=\"coerce\")\n",
    "df_data['total_no_of_videos'] = pd.to_numeric(df_data['total_no_of_videos'], errors=\"coerce\")\n",
    "df_data['total_no_short_videos'] = pd.to_numeric(df_data['total_no_short_videos'], errors=\"coerce\")\n",
    "df_data['total_no_long_videos'] = pd.to_numeric(df_data['total_no_long_videos'], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = df_data[df_data.isnull().any(axis=1)]\n",
    "print(null_rows)\n",
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['channel_start_date'] = pd.to_datetime(df_data['channel_start_date'], errors=\"coerce\")\n",
    "df_data['inception_date'] = pd.to_datetime(df_data['inception_date'], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reference date to now in UTC\n",
    "reference_date = pd.to_datetime(pd.Timestamp.now()).tz_localize('UTC')\n",
    "# Convert the channel start date and inception date to UTC\n",
    "df_data['channel_start_date'] = pd.to_datetime(df_data['channel_start_date']).dt.tz_localize('UTC')\n",
    "df_data['inception_date'] = pd.to_datetime(df_data['inception_date']).dt.tz_localize('UTC')\n",
    "# Now calculate days since start and inception\n",
    "df_data['days_since_start'] = (reference_date - df_data['channel_start_date']).dt.days\n",
    "df_data['days_since_inception'] = (reference_date - df_data['inception_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECTKBEST Feature for DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_data.drop(['channel_id', 'channel_name', 'channel_start_date', 'inception_date', 'total_subscribers'], axis=1)\n",
    "y = df_data['total_subscribers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature = SelectKBest(score_func = chi2, k = 11)\n",
    "fit = best_feature.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(fit.scores_, columns=['score'])\n",
    "column_df = pd.DataFrame(x.columns, columns=['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score_df = pd.concat([column_df, score_df], axis=1)\n",
    "# Convert the 'score' column to numeric\n",
    "feature_score_df['score'] = pd.to_numeric(feature_score_df['score'])\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "# Display the DataFrame\n",
    "print(feature_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HIGHER THE SCORE = More important the feature is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_views, total_likes, total_comments are the most important features in predicting the number of subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_score_df.nlargest(8, 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var = VarianceThreshold(threshold=0)\n",
    "var.fit(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold):\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(x, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cor = x.corr()\n",
    "sns.heatmap(cor, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "mi = mutual_info_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = pd.Series(mi)\n",
    "mi.index = x.columns\n",
    "mi.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=21, random_state=23)\n",
    "rf.fit(X_train[['total_views', 'total_likes', 'total_comments', 'total_no_of_videos', 'total_no_long_videos', 'days_since_start']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test[['total_views', 'total_likes', 'total_comments', 'total_no_of_videos', 'total_no_long_videos', 'days_since_start']], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
